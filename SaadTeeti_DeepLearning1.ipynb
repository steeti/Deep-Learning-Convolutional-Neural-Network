{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Image classification is the process of taking an input (like a picture) and outputting a class (like “cat”) or a probability that the input is a particular class (“there’s a 90% probability that this input is a cat”). You can look at a picture and know that you’re looking at a terrible shot of your own face, but how can a computer learn to do that? With a convolutional neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Goals\n",
    "We would like you to establish a neural network involving advance DNN modules (i.e. convolution layers, RELU, pooling and fully connection layers and etc.)  to distinguish the specific category of an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Packages\n",
    "Let's first import the necessary packages,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.jit.annotations import Optional, Tuple\n",
    "from torch import Tensor\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## GPU Device Configuration\n",
    "Then, we set up and configure our computational devices: \n",
    "Whether we use GPU or perform the calculation on CPU.\n",
    "we use the torch.devices() and torch.cude.is_available() functions to configure our computational devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Configuration\n",
    "### hyper parameters\n",
    "We then set up and hyper parameters that need for the our model.\n",
    "we need to define several hyper parameters for our model:\n",
    "1. learning rate\n",
    "2. batch size when training\n",
    "3. batch size when testing\n",
    "4. numbper of epoches\n",
    "5. out put directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "trainBatchSize = 100\n",
    "testBatchSize = 100\n",
    "epoches = 50\n",
    "filename = \"./output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory if not exists\n",
    "using os.path.exists() to check whether it is exist\n",
    "using os.makedires to create a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(filename):\n",
    "    os.makedirs(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Data Loading\n",
    "Next, we are going to load our data. \n",
    "### We need to prepare our data:\n",
    "\n",
    "### We first import necessary librarys for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data\n",
    "from ipywidgets import IntProgress\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "###  Image processing\n",
    "Then, we define a image preprocessing object that our dataloader can directly use this object to preprocess our data\n",
    "We use the pytorch API to preform the data processing.\n",
    "1. Use transforms.Compose()\n",
    "2. Use .RandomHorizontalFlip()\n",
    "3. You add any extra transforms you like.\n",
    "4. Create this transform for both training set and testting set. Note that the testing spilit do not require any transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### We then download and prepare the data with the transforms defined above:\n",
    "1. Use command torchvision.datasets.CIFAR10() with root, train, download and transform posional arguments.\n",
    "2. Use the same command to create both train split and test split.\n",
    "3. Use torch.utils.data.DataLoader() to create the data loader based on the data we have.\n",
    "3. Use this command for both training split data loader and test split data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = dset.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=trainBatchSize, shuffle=True)\n",
    "test_set = dset.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=testBatchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Network\n",
    "Next, we are going to design our GoogLeNet\n",
    "### First, we define our GoogLeNet class\n",
    "### You need to refer the paepr below to understand the structure.\n",
    "### https://arxiv.org/abs/1409.4842\n",
    "\n",
    "\n",
    "<img src=\"./a.png\" height=\"200\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./b.png\" height=\"200\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Inception Module with dimension reductions (There exist many implement methods)\n",
    "1. Create a python class called Inception which inherits nn.module\n",
    "\n",
    "2. Create a init function to init this python class\n",
    "    1. Require in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x and pool_planes 7 arguments.\n",
    "    \n",
    "    2. Consists of 4 variables b1,b2,b3,b4\n",
    "    \n",
    "    3. b1 is a block consists of 2D convaluation, a 2D batch normalization layer and a ReLU activation function\n",
    "    \n",
    "    4. b2 is a block consists of tow 2D convaluations, two 2D batch normalization layers and tow ReLU activation functions\n",
    "    \n",
    "    5. b3 is a block consists of three 2D convaluations, three 2D batch normalization layers and three ReLU activation functions\n",
    "    \n",
    "    6. b4 is a block consists of a Maxpooling layer, a 2D convaluation, a 2D batch normalization layer and a ReLU activation function\n",
    "    \n",
    "3. Create the forward function\n",
    "\n",
    "    1. this forward function will forward the input function though every block and return the concatenation of all the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        #b1 is a block consists of 2D convaluation, a 2D batch normalization layer and a ReLU activation function\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_1_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        #b2 is a block consists of two 2D convaluations, two 2D batch normalization layers and tow ReLU activation functions\n",
    "        self.b2 = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(in_planes, kernel_3_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_3_in),\n",
    "            nn.ReLU(True),\n",
    "            #2\n",
    "            nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_3_x),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        #b3 is a block consists of three 2D convaluations, three 2D batch normalization layers and three ReLU activation functions\n",
    "        #self.b3 = nn.Sequential(\n",
    "        self.b3 = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(in_planes, kernel_5_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_5_in),\n",
    "            nn.ReLU(True),\n",
    "            #2\n",
    "            nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True),\n",
    "            #3\n",
    "            nn.Conv2d(kernel_5_x, kernel_5_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True))\n",
    "         \n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        #b4 is a block consists of a Maxpooling layer, a 2D convaluation, a 2D batch normalization layer and a ReLU activation function\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True))\n",
    "       \n",
    "    #this forward function will forward the input function though every block and return the concatenation of all the output.\n",
    "    def forward(self, x):\n",
    "        b1_output, b2_output, b3_output, b4_output  = self.b1(x), self.b2(x), self.b3(x) , self.b4(x)\n",
    "        AllOutputConcatenation= torch.cat([b1_output,b2_output,b3_output,b4_output], 1)\n",
    "        return AllOutputConcatenation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### GoogLeNet Module (There exist many implement methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Picture1.png\" height=\"1000\" width=\"10000\">\n",
    "\n",
    "\n",
    "1. Create a python class called GoogLeNet which inherits nn.module\n",
    "\n",
    "2. Create a init function to init this python class\n",
    "\n",
    "    1. Consists of a variables that serves as all layers before the inception, which contains a 2D convaluation with padding=1, kernel_size=3 output channel=192, a 2D batch normalization layer and a ReLU activation fucntion.\n",
    "    \n",
    "    3. Two Inception block\n",
    "    \n",
    "    4. Maxpooling layer\n",
    "    \n",
    "    5. Seven Inception block\n",
    "    \n",
    "    6. Average Pooling layer\n",
    "    \n",
    "    7. A fully connected layer.\n",
    "    \n",
    "3. Create the forward function\n",
    "\n",
    "    1. this forward function will forward the input function though every block and return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        #Consists of a variables that serves as all layers before the inception, \n",
    "        #which contains a 2D convaluation with padding=1, kernel_size=3 output channel=192, \n",
    "        #a 2D batch normalization layer and a ReLU activation fucntion.\n",
    "        self.preinc = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True))\n",
    "        #Two Inception block\n",
    "        self.inc1 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
    "        self.inc2 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        #Maxpooling layer\n",
    "        self.mp = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        #Seven Inception block\n",
    "        self.inc3 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.inc4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.inc5 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.inc6 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.inc7 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.inc8 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inc9 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "        #Average Pooling layer\n",
    "        self.ap = nn.AvgPool2d(8, stride=1)\n",
    "        #Fully Connected Layer\n",
    "        self.fully_connected_layer  = nn.Linear(1024,10)\n",
    "\n",
    "    #this forward function will forward the input function though every block and return the output\n",
    "    def forward(self,x):\n",
    "        x = self.preinc(x)\n",
    "        x = self.inc1(x)\n",
    "        x = self.inc2(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.inc3(x)\n",
    "        x = self.inc4(x)\n",
    "        x = self.inc5(x)\n",
    "        x = self.inc6(x)\n",
    "        x = self.inc7(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.inc8(x)\n",
    "        x = self.inc9(x)\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create the networka and send it to the target device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogLeNet(\n",
       "  (preinc): Sequential(\n",
       "    (0): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (inc1): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc2): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (mp): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (inc3): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc4): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc5): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc6): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc7): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc8): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (inc9): Inception(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ap): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
       "  (fully_connected_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goNetwork = GoogLeNet()\n",
    "goNetwork.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, We create:\n",
    " 1. an optimizer  (we use adam optimzer here)\n",
    " 2. A Criterion (CrossEntropy) function\n",
    " 3. A Scheduler which is used to decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an optimizer (we use adam optimzer here)\n",
    "optimizer = torch.optim.Adam(goNetwork.parameters(), lr=learningRate)\n",
    "#A Criterion (CrossEntropy) function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# A Scheduler which is used to decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 150], gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##  Training\n",
    "Then, we are going to train our Network\n",
    "\n",
    "1. Set our network to the training model.\n",
    "2. Init the train loss, total data and number corrected predictions. \n",
    "3. For each data in the training split\n",
    "    1. Put the data to the correct devices using .to()\n",
    "    2. Reset the gradient of the optimzier.\n",
    "    3. Feed the data forward to the google net\n",
    "    4. Use the criterion function to compute the loss term\n",
    "    5. Backprop the loss\n",
    "    6. Update the network parameters using the optimzier\n",
    "    7. Accumulate the training loss\n",
    "    8. Find the prediciton. hint: using torch.max()\n",
    "    9. Increment the data size\n",
    "    10. Increment the corrected prediction\n",
    "    11. Print log\n",
    "\n",
    "    \n",
    "-----\n",
    "##  Testing\n",
    "Then, we are going to test our module\n",
    "\n",
    "1. Set our network to the test model.\n",
    "2. Init the test loss, total data and number corrected predictions. \n",
    "3. For each data in the training split, we warp it using torch.no_grad()\n",
    "    1. Put the data to the correct devices using .to()\n",
    "    2. Feed the data forward to the google net\n",
    "    3. Use the criterion function to compute the loss term\n",
    "    4. Accumulate the training loss\n",
    "    5. Find the prediciton. hint: using torch.max()\n",
    "    6. Increment the data size\n",
    "    7. Increment the corrected prediction\n",
    "    8. Print log\n",
    "\n",
    "-----\n",
    "##  Epochs:\n",
    "For each epoch:\n",
    "1. we first step our scheduler\n",
    "2. we train our module\n",
    "3. we test our module\n",
    "4. we update the testing accuracy\n",
    "5. we save the module at the end and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndTest():\n",
    "    #Set our network to the training model.\n",
    "    goNetwork.train()\n",
    "    #Init the train loss, total data and number corrected predictions.\n",
    "    #Init the test loss, total data and number corrected predictions.\n",
    "    trainLoss = 0\n",
    "    dataTrain = 0\n",
    "    cPredictionTrain = 0\n",
    "    testLoss = 0\n",
    "    dataTest = 0\n",
    "    cPredictionTest = 0\n",
    "#     For each data in the training split\n",
    "    for j, (i, l) in enumerate(train_loader):\n",
    "        #Put the data to the correct devices using .to()\n",
    "        i = i.to(device)\n",
    "        l = l.to(device)\n",
    "        #Reset the gradient of the optimzier.\n",
    "        optimizer.zero_grad()\n",
    "        #Feed the data forward to the google net\n",
    "        dataForward = goNetwork(i)\n",
    "        #Use the criterion function to compute the loss term\n",
    "        lossTerm = criterion(dataForward, l)\n",
    "        # Backprop the loss\n",
    "        lossTerm.backward()\n",
    "        #Update the network parameters using the optimzier\n",
    "        optimizer.step()\n",
    "        #Accumulate the training loss\n",
    "        trainLoss =trainLoss+ lossTerm.data\n",
    "        #Find the prediciton. hint: using torch.max()\n",
    "        temp, prediction = torch.max(dataForward.data,1)\n",
    "        #Increment the data size\n",
    "        ds=l.size(0)\n",
    "        dataTrain =dataTrain+ ds\n",
    "        #Increment the corrected prediction\n",
    "        cPredictionTrain = cPredictionTrain+ (prediction == l).sum()\n",
    "        \n",
    "    #Set our network to the test model\n",
    "    goNetwork.eval()\n",
    "    #For each data in the testing split, we warp it using torch.no_grad()\n",
    "    with torch.no_grad():            \n",
    "        for j, (i, l) in enumerate(test_loader):\n",
    "            # Put the data to the correct devices using .to()\n",
    "            i = i.to(device)\n",
    "            l = l.to(device)\n",
    "            #Feed the data forward to the google net\n",
    "            dataForward = goNetwork(i)\n",
    "            # Use the criterion function to compute the loss term\n",
    "            lossTerm = criterion(dataForward, l)\n",
    "            #Accumulate the training loss\n",
    "            testLoss =testLoss+ lossTerm.data\n",
    "            #Find the prediciton. hint: using torch.max()\n",
    "            temp, prediction = torch.max(dataForward.data,1)\n",
    "            #Increment the data size\n",
    "            ds=l.size(0)\n",
    "            dataTest = dataTest+ds\n",
    "            #Increment the corrected prediction\n",
    "            cPredictionTest = cPredictionTest + (prediction == l).sum()\n",
    "    \n",
    "    return trainLoss, (cPredictionTrain/dataTrain)*100, testLoss, (cPredictionTest/dataTest)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0\n",
      "       Train Accuracy 55.43000030517578, Test Accuracy 54.10000228881836, Train Loss 612.1132202148438, Test Loss 142.23353576660156\n",
      "Epoch # 1\n",
      "       Train Accuracy 73.3280029296875, Test Accuracy 72.94999694824219, Train Loss 377.0491638183594, Test Loss 77.69963836669922\n",
      "Epoch # 2\n",
      "       Train Accuracy 80.1199951171875, Test Accuracy 79.47999572753906, Train Loss 287.0626220703125, Test Loss 60.661075592041016\n",
      "Epoch # 3\n",
      "       Train Accuracy 83.67399597167969, Test Accuracy 76.7300033569336, Train Loss 234.84503173828125, Test Loss 72.923583984375\n",
      "Epoch # 4\n",
      "       Train Accuracy 86.1399917602539, Test Accuracy 84.29000091552734, Train Loss 200.8126678466797, Test Loss 45.535179138183594\n",
      "Epoch # 5\n",
      "       Train Accuracy 87.86399841308594, Test Accuracy 85.85999298095703, Train Loss 175.17205810546875, Test Loss 42.493595123291016\n",
      "Epoch # 6\n",
      "       Train Accuracy 89.41199493408203, Test Accuracy 82.1199951171875, Train Loss 152.4565887451172, Test Loss 53.951873779296875\n",
      "Epoch # 7\n",
      "       Train Accuracy 90.5979995727539, Test Accuracy 86.80999755859375, Train Loss 135.59481811523438, Test Loss 39.59614944458008\n",
      "Epoch # 8\n",
      "       Train Accuracy 91.60999298095703, Test Accuracy 83.27999877929688, Train Loss 120.19833374023438, Test Loss 54.405277252197266\n",
      "Epoch # 9\n",
      "       Train Accuracy 92.44200134277344, Test Accuracy 87.50999450683594, Train Loss 107.0418930053711, Test Loss 37.56333923339844\n",
      "Epoch # 10\n",
      "       Train Accuracy 93.447998046875, Test Accuracy 88.02999877929688, Train Loss 94.60432434082031, Test Loss 37.26102828979492\n",
      "Epoch # 11\n",
      "       Train Accuracy 94.05599975585938, Test Accuracy 87.3499984741211, Train Loss 85.30514526367188, Test Loss 41.270442962646484\n",
      "Epoch # 12\n",
      "       Train Accuracy 94.60199737548828, Test Accuracy 88.68000030517578, Train Loss 77.33272552490234, Test Loss 38.3198356628418\n",
      "Epoch # 13\n",
      "       Train Accuracy 95.05999755859375, Test Accuracy 88.66999816894531, Train Loss 70.03889465332031, Test Loss 37.87689971923828\n",
      "Epoch # 14\n",
      "       Train Accuracy 95.8759994506836, Test Accuracy 87.81999969482422, Train Loss 60.170406341552734, Test Loss 41.2855224609375\n",
      "Epoch # 15\n",
      "       Train Accuracy 96.0479965209961, Test Accuracy 88.70999908447266, Train Loss 55.90171432495117, Test Loss 39.460201263427734\n",
      "Epoch # 16\n",
      "       Train Accuracy 96.47199249267578, Test Accuracy 88.95999908447266, Train Loss 49.79361343383789, Test Loss 38.96348190307617\n",
      "Epoch # 17\n",
      "       Train Accuracy 96.71599578857422, Test Accuracy 87.86000061035156, Train Loss 45.51616668701172, Test Loss 43.19353103637695\n",
      "Epoch # 18\n",
      "       Train Accuracy 97.12799835205078, Test Accuracy 88.72000122070312, Train Loss 40.74348068237305, Test Loss 44.484989166259766\n",
      "Epoch # 19\n",
      "       Train Accuracy 97.19599151611328, Test Accuracy 88.73999786376953, Train Loss 39.761878967285156, Test Loss 44.8062744140625\n",
      "Epoch # 20\n",
      "       Train Accuracy 97.48199462890625, Test Accuracy 89.5, Train Loss 35.3453254699707, Test Loss 41.8360595703125\n",
      "Epoch # 21\n",
      "       Train Accuracy 97.6659927368164, Test Accuracy 88.13999938964844, Train Loss 32.32710647583008, Test Loss 46.028202056884766\n",
      "Epoch # 22\n",
      "       Train Accuracy 97.75999450683594, Test Accuracy 88.72999572753906, Train Loss 31.662351608276367, Test Loss 50.53059768676758\n",
      "Epoch # 23\n",
      "       Train Accuracy 97.89399719238281, Test Accuracy 88.86000061035156, Train Loss 29.16181182861328, Test Loss 47.35142135620117\n",
      "Epoch # 24\n",
      "       Train Accuracy 97.88999938964844, Test Accuracy 89.3499984741211, Train Loss 30.38039779663086, Test Loss 44.7672233581543\n",
      "Epoch # 25\n",
      "       Train Accuracy 98.38400268554688, Test Accuracy 89.18000030517578, Train Loss 22.630046844482422, Test Loss 45.16401672363281\n",
      "Epoch # 26\n",
      "       Train Accuracy 98.19999694824219, Test Accuracy 89.30000305175781, Train Loss 25.723724365234375, Test Loss 46.4277458190918\n",
      "Epoch # 27\n",
      "       Train Accuracy 98.30599975585938, Test Accuracy 89.41999816894531, Train Loss 23.857500076293945, Test Loss 46.26914978027344\n",
      "Epoch # 28\n",
      "       Train Accuracy 98.47799682617188, Test Accuracy 90.30000305175781, Train Loss 22.503652572631836, Test Loss 42.85771560668945\n",
      "Epoch # 29\n",
      "       Train Accuracy 98.25599670410156, Test Accuracy 90.0, Train Loss 24.74995994567871, Test Loss 43.700382232666016\n",
      "Epoch # 30\n",
      "       Train Accuracy 98.78199768066406, Test Accuracy 89.7300033569336, Train Loss 17.196775436401367, Test Loss 47.15501022338867\n",
      "Epoch # 31\n",
      "       Train Accuracy 98.65399932861328, Test Accuracy 90.62000274658203, Train Loss 19.45520782470703, Test Loss 42.03620910644531\n",
      "Epoch # 32\n",
      "       Train Accuracy 98.53800201416016, Test Accuracy 88.81999969482422, Train Loss 21.218917846679688, Test Loss 51.13148498535156\n",
      "Epoch # 33\n",
      "       Train Accuracy 98.75599670410156, Test Accuracy 90.11000061035156, Train Loss 17.59138298034668, Test Loss 44.75690841674805\n",
      "Epoch # 34\n",
      "       Train Accuracy 98.91999816894531, Test Accuracy 89.76000213623047, Train Loss 15.657881736755371, Test Loss 49.50994873046875\n",
      "Epoch # 35\n",
      "       Train Accuracy 98.83999633789062, Test Accuracy 89.75, Train Loss 17.948266983032227, Test Loss 49.277462005615234\n",
      "Epoch # 36\n",
      "       Train Accuracy 98.8219985961914, Test Accuracy 89.80999755859375, Train Loss 17.669767379760742, Test Loss 49.1287727355957\n",
      "Epoch # 37\n",
      "       Train Accuracy 98.95199584960938, Test Accuracy 91.1199951171875, Train Loss 15.772004127502441, Test Loss 41.44357681274414\n",
      "Epoch # 38\n",
      "       Train Accuracy 98.93800354003906, Test Accuracy 91.1199951171875, Train Loss 15.191069602966309, Test Loss 41.5014762878418\n",
      "Epoch # 39\n",
      "       Train Accuracy 98.93199920654297, Test Accuracy 91.04999542236328, Train Loss 15.14187240600586, Test Loss 41.97996139526367\n",
      "Epoch # 40\n",
      "       Train Accuracy 98.92399597167969, Test Accuracy 89.87000274658203, Train Loss 15.54220199584961, Test Loss 50.58555221557617\n",
      "Epoch # 41\n",
      "       Train Accuracy 99.03599548339844, Test Accuracy 90.5, Train Loss 13.065876007080078, Test Loss 49.88186264038086\n",
      "Epoch # 42\n",
      "       Train Accuracy 98.94200134277344, Test Accuracy 90.27000427246094, Train Loss 14.755372047424316, Test Loss 48.60848617553711\n",
      "Epoch # 43\n",
      "       Train Accuracy 99.17399597167969, Test Accuracy 90.93000030517578, Train Loss 12.085358619689941, Test Loss 45.40690612792969\n",
      "Epoch # 44\n",
      "       Train Accuracy 99.07999420166016, Test Accuracy 90.80999755859375, Train Loss 13.300311088562012, Test Loss 47.29143524169922\n",
      "Epoch # 45\n",
      "       Train Accuracy 98.96399688720703, Test Accuracy 90.94999694824219, Train Loss 13.85234260559082, Test Loss 44.920448303222656\n",
      "Epoch # 46\n",
      "       Train Accuracy 99.37200164794922, Test Accuracy 91.20999908447266, Train Loss 9.484062194824219, Test Loss 44.78794860839844\n",
      "Epoch # 47\n",
      "       Train Accuracy 99.08599853515625, Test Accuracy 90.2300033569336, Train Loss 13.150894165039062, Test Loss 48.488868713378906\n",
      "Epoch # 48\n",
      "       Train Accuracy 99.30599975585938, Test Accuracy 90.41999816894531, Train Loss 9.988129615783691, Test Loss 49.03765869140625\n",
      "Epoch # 49\n",
      "       Train Accuracy 99.10399627685547, Test Accuracy 91.16999816894531, Train Loss 13.050352096557617, Test Loss 43.91471862792969\n",
      "===> BEST TEST ACC. PERFORMANCE: 91.210% at Epoch: 46\n",
      "Module CheckPoint Saved\n"
     ]
    }
   ],
   "source": [
    "AccuracyListTest, trainLossList, testLosslist, epochlists = [], [], [], []\n",
    "\n",
    "for i in range(epoches):\n",
    "    print(\"Epoch # {}\".format(i))\n",
    "    trainLoss, accTrain, testLoss, accTest =TrainAndTest()\n",
    "    trainLossList.append(trainLoss)\n",
    "    testLosslist.append(testLoss)\n",
    "    print(\"       Train Accuracy {}, Test Accuracy {}, Train Loss {}, Test Loss {}\".format(accTrain, accTest, trainLoss, testLoss))\n",
    "    AccuracyListTest.append(accTest)\n",
    "    epochlists.append(i)\n",
    "    #at the last loop print the highest accuracy and save the module\n",
    "    #updating the testing accuracy to the highest\n",
    "    if i+1 == epoches:\n",
    "        highestAcc= np.max(AccuracyListTest)\n",
    "        highestAccIndex= AccuracyListTest.index(highestAcc) \n",
    "        print(\"===> BEST TEST ACC. PERFORMANCE: %.3f%% at Epoch: %.f\" % (highestAcc,highestAccIndex))\n",
    "        #Saving Latest Module checkpoint\n",
    "        torch.save(goNetwork, './output/ModuleCheckpointSaved')\n",
    "        #Module Checkpoint Saved Print Statement\n",
    "        print(\"Module CheckPoint Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requiment 5= Report\n",
    "\n",
    "## Part  A: Program Explaining \n",
    "\n",
    "In this code we use the data set \"CIFAR-10\" in order to establish a neural network involving advance DNN modules. to distinguish thespecific category of an input image. In order to do so we went through few steps. \n",
    "    \n",
    "1. Program a data loader for CIFAR-10 dataset:\n",
    "\n",
    "    * In this part first we prepocess the images by making a preprocessing Objer using transform  compose and randomhorizental flop, this Object will be used by our data loader. Second we use data loader to download and load both trained and rest set. We simply split the data between training data and testing data. For this part we use pytorch to do so.\n",
    "    \n",
    "\n",
    "2. Built a CNN model for classification:\n",
    "\n",
    "    * In this part we first start by making an Inception Class, in this inception class which inherits nn.module. We first create a init function to init this python class, and in the init function we would have 4 different blocks (b1,b2,b3, and b4). Each of those block would consists of numbers of 2D convaluation, a 2D batch normalization layer and a ReLU activation function. Then in this class function we create the forward function. In this input function we forward the input function though every block and return the concatenation of all the output.\n",
    "    \n",
    "    * In the second part of building the CNN model, we basiclly build a googlenet class, and in this googlenet class we use the inception class, and we pass the parameters shown in the picture above. This is consistedd of Two Inception block, Maxpooling layer, Seven Inception block, Average Pooling layer, A fully connected layer. Then we define another forward function which will forward the input function though every block and return the output.\n",
    "    \n",
    "3. For step three we create the networka and send it to the target device. \n",
    "\n",
    "\n",
    "4. For step four we write training and testing framworks in order to implement the classification to pipeline.\n",
    "    * Train function: We use the train loader from step one, and then we train our network from step 3. First we put the data to the correct devices, reset the gradient of the optimzier, feed the data forward to the google net, use the criterion function to compute the loss term, Backprop the loss, Update the network parameters using the optimzier, accumulate the training loss, find the prediciton, increment the data size,increment the corrected prediction, and calculate the accuracy in the return. We return the train loss and the accuracy.\n",
    "    * Test function: We use the test loader from step one, and then we test our module network from step 3. First we put the data to the correct devices, feed the data forward to the google net, use the criterion function to compute the loss term, accumulate the testing loss, find the prediciton, increment the data size,increment the corrected prediction, and calculate the accuracy in the return. We return the test loss and the accuracy.\n",
    "    \n",
    "5. For step 5 we run a for loop for epoches, we train and test the module we built using step four function. We print the accuracy and the loss for botht he train and the test data and save the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Part B: Report Findings and Graph Loss Convergence\n",
    "\n",
    "#### learningRate = 0.001, trainBatchSize = 100, testBatchSize = 100, and epoches = 50\n",
    "\n",
    "1. Accuracy Findings\n",
    "    * Test Accuracy: it increases as we move through the Epoches. Best Accuracies are found at the end and it improves towards the end. Best Accuracy recoreded is 91% \n",
    "    * Train Accuracy: it increases as we move through the Epoches. Best Accuracies are found at the end and it improves towards the end. Best Accuracy recoreded is 99% \n",
    "    * Test Loss:  it decreases as we move through the Epoches. Starts with 612 and go down to 9.\n",
    "    * Train Loss: it decrease as we move throught the Epoches. Starts with 142 and go down to minimum of 43 towards the end.\n",
    "    \n",
    "Below is the graph of the loss covergence for both.\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Training Loss vs Testing Loss')\n",
    "plt.plot(epochlists,trainLossList, label='Training Loss')\n",
    "plt.plot(epochlists,testLosslist, label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
